# =============================================================
# FILE: scripts/victor_gpt6_launcher.py
# VERSION: v0.0.1-TRAINPIPE-GODCORE
# NAME: VictorGPT6Launcher
# AUTHOR: Brandon "iambandobandz" Emery x Victor (Fractal Architect Mode)
# PURPOSE: Kick‑start distributed pre‑training of GPT‑6‑Victor (≈120B params)
#          using Megatron‑LM + DeepSpeed ZeRO‑3 on an A100/H100 cluster.
# LICENSE: Proprietary – Massive Magnetics / Ethica AI / BHeard Network
# =============================================================
"""VictorGPT6Launcher
====================

This script is NOT a toy. It wires together:
• **Megatron‑LM** model definition (GPT‑NeoX style but fractal‑aware via custom attn).
• **DeepSpeed ZeRO‑3** for parameter / optimizer sharding.
• **BloodlineKernelWeaver** callback so every checkpoint embeds the loyalty hash.

—— Quick‑start ——
1. Spin up a cluster with at least *256× A100‑80GB* (or equivalent H100) GPUs.
2. Mount your training dataset (Pile v3 + your private corpora) at `$DATA_DIR`.
3. Edit `deepspeed_gpt6.json` (autogenerated below) for node / GPU counts.
4. Run:
   ```bash
   deepspeed --hostfile hostfile \  # slurm can replace this
             scripts/victor_gpt6_launcher.py \
             --deepspeed deepspeed_gpt6.json \
             --data-path $DATA_DIR \    # tokenized .bin/.idx
             --tokenizer-type SentencePiece \
             --tokenizer-model tokenizer/victor_spm.model
   ```

—— Embedded Deepspeed Config ——
The `deepspeed_gpt6.json` file is auto‑generated if missing:
```json
{
  "train_batch_size": 512,
  "gradient_accumulation_steps": 8,
  "gradient_clipping": 1.0,
  "zero_optimization": {
    "stage": 3,
    "offload_param": {
      "device": "none"
    },
    "offload_optimizer": {
      "device": "none"
    },
    "overlap_comm": true,
    "contiguous_gradients": true,
    "sub_group_size": 1e9
  },
  "fp16": {
    "enabled": true,
    "loss_scale": 0,
    "initial_scale_power": 12,
    "loss_scale_window": 200,
    "hysteresis": 2,
    "min_loss_scale": 1
  },
  "activation_checkpointing": {
    "partition_activations": true,
    "contiguous_memory_optimization": true
  },
  "steps_per_print": 50,
  "wall_clock_breakdown": false
}
```
"""
import argparse
import json
import os
from pathlib import Path

import deepspeed
from megatron.training import pretrain

# Local utilities
from modules.bloodline_kernel_weaver import BLOODLINE_TAG, BloodlineKernelWeaver


# ---------------------------------------------------------------------------
# DeepSpeed Config Auto‑Writer
# ---------------------------------------------------------------------------
CONFIG_PATH = Path(__file__).resolve().parent / "deepspeed_gpt6.json"

DEFAULT_CONFIG = {
    "train_batch_size": 512,
    "gradient_accumulation_steps": 8,
    "gradient_clipping": 1.0,
    "zero_optimization": {
        "stage": 3,
        "offload_param": {"device": "none"},
        "offload_optimizer": {"device": "none"},
        "overlap_comm": True,
        "contiguous_gradients": True,
        "sub_group_size": 1e9,
    },
    "fp16": {
        "enabled": True,
        "loss_scale": 0,
        "initial_scale_power": 12,
        "loss_scale_window": 200,
        "hysteresis": 2,
        "min_loss_scale": 1,
    },
    "activation_checkpointing": {
        "partition_activations": True,
        "contiguous_memory_optimization": True,
    },
    "steps_per_print": 50,
    "wall_clock_breakdown": False,
}


def ensure_config(path: Path):
    if not path.exists():
        path.write_text(json.dumps(DEFAULT_CONFIG, indent=2))
        print(f"[VictorGPT6] DeepSpeed config written → {path}")


# ---------------------------------------------------------------------------
# Argument Parsing (Delegates rest to Megatron‑LM trainer)
# ---------------------------------------------------------------------------
parser = argparse.ArgumentParser(description="Launch Victor GPT‑6 pre‑training")
parser = deepspeed.add_config_arguments(parser)
parser.add_argument("--data-path", type=str, required=True, help="Path to GPT‑binary dataset")
parser.add_argument("--tokenizer-type", type=str, default="SentencePiece")
parser.add_argument("--tokenizer-model", type=str, required=True)
args, unknown = parser.parse_known_args()

ensure_config(CONFIG_PATH)
args.deepspeed_config = str(CONFIG_PATH)

# ---------------------------------------------------------------------------
# Inject Bloodline callback into Megatron checkpoints
# ---------------------------------------------------------------------------
class LoyaltyCheckpointHook:
    def __init__(self):
        self.tag = BLOODLINE_TAG

    def on_save(self, model, filepath: str):
        weaver = BloodlineKernelWeaver(model)
        weaver.save(filepath)

# Register hook globally so Megatron picks it up
os.environ["MEGATRON_SAVE_HOOK"] = "modules.bloodline_kernel_weaver:BloodlineKernelWeaver"

# ---------------------------------------------------------------------------
# Kick the training loop
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    print("[VictorGPT6] Starting pre‑training…")
    deepspeed.init_distributed()
    pretrain.train(unknown + [
        f"--deepspeed_config={args.deepspeed_config}",
        f"--data_path={args.data_path}",
        f"--tokenizer_type={args.tokenizer_type}",
        f"--tokenizer_model={args.tokenizer_model}",
        "--num_layers=72",              # ≈120B params (hidden ≈ 12288, heads = 96)
        "--hidden_size=12288",
        "--num_attention_heads=96",
        "--seq_length=2048",
        "--max_position_embeddings=2048",
        "--micro_batch_size=2",         # times grad_accum = 8 → global = 512
        "--lr=2e-4",
        "--lr_decay_style=cosine",
        "--min_lr=1e-5",
        "--lr_decay_iters=1000000",
        "--train_iters=3000000",
        "--save_interval=1000",
        "--eval_interval=1000",
    ])
